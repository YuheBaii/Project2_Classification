#!/usr/bin/env python3
import os
import random
import json
from pathlib import Path
import yaml

import torch
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from torch.utils.data import DataLoader # ç¡®ä¿ DataLoader å…¨å±€å¯ç”¨

# Optional: try to import seaborn for nicer heatmap; fallback to sklearn plotting
try:
    import seaborn as sns # type: ignore
    _HAS_SEABORN = True
except Exception:
    _HAS_SEABORN = False


def load_model_and_data(cfg_path, checkpoint_path, device="cuda"):
    """
    åŠ è½½æ¨¡åž‹ä¸Ž dataloaderã€‚è¿”å›ž model, dataloader, ckpt_dict, class_names
    éœ€è¦é¡¹ç›®ä¸­æä¾› build_transforms, build_data, build_model æŽ¥å£
    """
    # --- è¯»å– YAML é…ç½® ---
    cfg_path = Path(cfg_path)
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config file not found: {cfg_path}")

    with open(cfg_path, "r") as f:
        # å‡è®¾é…ç½®å¯èƒ½æ˜¯ JSON æˆ– YAML
        if cfg_path.suffix in ['.yaml', '.yml']:
            cfg = yaml.safe_load(f)
        else:
            cfg = json.load(f)

    # --- å¯¼å…¥é¡¹ç›®å†…éƒ¨æž„å»ºå‡½æ•°ï¼ˆæ ¹æ®å®žé™…è·¯å¾„è°ƒæ•´ï¼‰ ---
    try:
        # ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®ï¼Œè¿™é‡Œä¿æŒç”¨æˆ·åŽŸå§‹çš„å¯¼å…¥æ–¹å¼
        from src.transforms.build import build_transforms, build_data, build_model
    except Exception as e:
        raise ImportError(
            "æ— æ³•å¯¼å…¥é¡¹ç›®å†…éƒ¨æž„å»ºå‡½æ•° (build_transforms/build_data/build_model)ã€‚\n"
            "è¯·ç¡®è®¤æ¨¡å—è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–åœ¨æ­¤è„šæœ¬ä¸­è°ƒæ•´å¯¼å…¥è·¯å¾„ã€‚\n"
            f"åŽŸå§‹é”™è¯¯: {e}"
        )

    # ðŸš¨ å…³é”®ä¿®å¤ç‚¹ Aï¼šåœ¨æ¨¡åž‹æž„å»ºå’Œæƒé‡æ£€æŸ¥ä¹‹å‰åŠ è½½ Checkpoint
    checkpoint_path = Path(checkpoint_path)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    ckpt = torch.load(checkpoint_path, map_location=device)
    print(f"Loaded checkpoint from {checkpoint_path}")

    # --- æž„å»º transforms / datamodule ---
    train_tf, val_tf = build_transforms(cfg.get("augmentation", None))
    # ðŸš¨ å…³é”®ä¿®å¤ç‚¹ Bï¼šæŽ¥æ”¶ build_data è¿”å›žçš„ 3 ä¸ªå€¼
    datamodule, class_names, class_weights = build_data(cfg, train_tf, val_tf)

    # --- èŽ·å–æœ€ç»ˆçš„ class_weights ---
    # æ£€æŸ¥ DataModule æ˜¯å¦æä¾›äº†æƒé‡ï¼Œå¦åˆ™å°è¯•ä»Ž Checkpoint ä¸­èŽ·å–
    # è¿™è§£å†³äº† 'class_weights is None and 'class_weights' in ckpt' å¤„çš„ UnboundLocalError
    if class_weights is None and 'class_weights' in ckpt:
        class_weights = ckpt['class_weights']
        print("Found class_weights in checkpoint and using it.")
    
    # --- æž„å»ºæ¨¡åž‹ ---
    num_classes = len(class_names)
    # ðŸš¨ å…³é”®ä¿®å¤ç‚¹ Cï¼šå°† class_weights ä¼ é€’ç»™ build_model
    model = build_model(cfg, num_classes=num_classes, class_weights=class_weights)
    model.to(device)


    # ã€å…³é”®ä¿®æ”¹åŒºåŸŸï¼šæ‰‹åŠ¨æž„å»º dataloader ä»¥ç¡®ä¿ transforms å·²åº”ç”¨ã€‘
    dataloader = None
    target_dataset = None
    
    # 1. å°è¯•èŽ·å– test_dataset (æœ€ä¼˜å…ˆç”¨äºŽæœ€ç»ˆè¯„ä¼°)
    if hasattr(datamodule, "test_dataset") and datamodule.test_dataset is not None:
        target_dataset = datamodule.test_dataset
        print("Using datamodule's test_dataset for inference.")
    # 2. å…¶æ¬¡å°è¯•èŽ·å– val_dataset
    elif hasattr(datamodule, "val_dataset") and datamodule.val_dataset is not None:
        target_dataset = datamodule.val_dataset
        print("Using datamodule's val_dataset for inference.")
        
    # å¦‚æžœæ‰¾åˆ°äº†æ•°æ®é›†ï¼Œåˆ™æ‰‹åŠ¨åˆ›å»º DataLoader
    if target_dataset is not None:
        # ä»Žå…¨å±€å¯¼å…¥ DataLoader (ç¡®ä¿ NameError ä¸å†å‘ç”Ÿ)
        bs = cfg.get("batch_size", 128)
        nw = cfg.get("num_workers", 4)
        
        dataloader = DataLoader(
            target_dataset,
            batch_size=bs,
            shuffle=False,
            num_workers=nw,
            pin_memory=True,
        )
        print(f"Manually created DataLoader with batch_size={bs}, num_workers={nw}")

    # ã€å›žé€€é€»è¾‘ï¼šå¦‚æžœ DataModule æ²¡æœ‰å…¬å¼€ dataset å±žæ€§ã€‘
    if dataloader is None:
        print("Dataset properties not found, falling back to dataloader() method.")
        for fn in ["test_dataloader", "val_dataloader", "train_dataloader"]:
            if hasattr(datamodule, fn):
                dl = getattr(datamodule, fn)()
                if dl is not None:
                    dataloader = dl
                    print(f"Fallback: Using dataloader via: {fn}()")
                    break
    
    # ã€ç»“æŸå…³é”®ä¿®æ”¹åŒºåŸŸã€‘

    if dataloader is None:
        raise ValueError(
            "æ— æ³•ä»Ž datamodule èŽ·å– dataloaderï¼Œè¯·æ£€æŸ¥ dataset è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"
        )
    
    # --- åŠ è½½ checkpoint å¹¶å¤„ç†å¯èƒ½çš„ key å‰ç¼€ ---
    # ç¡®ä¿ class_weights å·²åœ¨æ¨¡åž‹æž„å»ºæ—¶è¢«ä½¿ç”¨
    
    # ä»Ž checkpoint ä¸­æå– class_namesï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
    ckpt_class_names = ckpt.get("class_names", None)
    if ckpt_class_names:
        class_names = ckpt_class_names

    state_dict = ckpt.get("model", ckpt)
    if isinstance(state_dict, dict) and "state_dict" in state_dict:
        state_dict = state_dict["state_dict"]

    # åŽ»æŽ‰ 'module.' å‰ç¼€ï¼ˆå¸¸è§äºŽ DataParallel ä¿å­˜ï¼‰
    new_state = {k.replace("module.", "") if k.startswith("module.") else k: v
                 for k, v in state_dict.items()}

    try:
        # å°è¯•ä¸¥æ ¼åŠ è½½
        model.load_state_dict(new_state)
    except RuntimeError as e:
        # ðŸš¨ ä¿æŒ strict=False ä¿®å¤ä»¥åº”å¯¹ loss_fn.weight ç­‰ä¸åŒ¹é…é”®
        model.load_state_dict(new_state, strict=False)
        print("Warning: loaded state_dict with strict=False due to mismatch:", e)

    print(f"Loaded model from {checkpoint_path}")
    print(f"Classes: {class_names}")
    print(f"Checkpoint meta: acc={ckpt.get('acc', 'N/A')}, epoch={ckpt.get('epoch', 'N/A')}")

    return model, dataloader, ckpt, class_names

def _batch_to_images_and_labels(batch):
    """å…¼å®¹ä¸åŒ dataloader è¿”å›žç»“æž„ï¼š(images, labels) æˆ– dict"""
    if isinstance(batch, (list, tuple)) and len(batch) >= 2:
        images, labels = batch[0], batch[1]
    elif isinstance(batch, dict):
        images = batch.get("images") or batch.get("img") or batch.get("input")
        labels = batch.get("labels") or batch.get("targets")
        if images is None or labels is None:
            raise ValueError("æ— æ³•ä»Ž dict batch ä¸­è§£æž images/labels å­—æ®µã€‚")
    else:
        raise ValueError("Unknown batch format from dataloader.")
    return images, labels


def analyze_predictions(model, dataloader, class_names, device='cuda'):
    """åˆ†æžæ¨¡åž‹é¢„æµ‹ç»“æžœï¼ˆè‡ªåŠ¨è·³è¿‡åå›¾ç‰‡æˆ–å¼‚å¸¸ batchï¼‰"""
    model.eval()
    all_preds, all_labels, all_probs, all_images = [], [], [], []

    total = len(dataloader)
    print(f"å¼€å§‹æŽ¨ç†ï¼Œå…± {total} ä¸ª batch...")

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            try:
                # å…¼å®¹ä¸åŒæ ¼å¼
                images, labels = _batch_to_images_and_labels(batch)

                if images is None or labels is None:
                    print(f"âš ï¸ ç¬¬ {batch_idx} ä¸ª batch æ— æ•ˆï¼Œè·³è¿‡ã€‚")
                    continue

                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                # å¯¹äºŽå¤šåˆ†ç±»ï¼Œé€šå¸¸ä½¿ç”¨ softmax
                if outputs.dim() == 2 and outputs.shape[1] > 1:
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(probs, dim=1)
                elif outputs.dim() == 2 and outputs.shape[1] == 1: # å¯èƒ½æ˜¯äºŒåˆ†ç±»ï¼Œè¾“å‡ºä¸º (N, 1)
                    # å‡è®¾ BCEWithLogitsLoss çš„è¾“å‡ºï¼Œä½¿ç”¨ sigmoid
                    probs = torch.sigmoid(outputs).squeeze(1) # (N,)
                    preds = (probs > 0.5).long()
                else:
                    # é»˜è®¤å¤šåˆ†ç±»
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(probs, dim=1)


                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                # å¯¹äºŽäºŒåˆ†ç±» (N,) çš„ probsï¼Œéœ€è¦æ‰©å±•ç»´åº¦æ‰èƒ½ç”¨ np.array(all_probs)
                if probs.dim() == 1:
                    # è½¬æ¢ä¸º (N, 1) æ ¼å¼ï¼Œæˆ–åªå­˜å‚¨ (N, 1) çš„é¢„æµ‹æ¦‚çŽ‡
                    all_probs.extend(probs.unsqueeze(1).cpu().numpy())
                else:
                    all_probs.extend(probs.cpu().numpy())

                all_images.extend(images.cpu())

            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ç¬¬ {batch_idx} ä¸ª batchï¼ˆå¯èƒ½åŒ…å«æŸåå›¾ç‰‡æˆ–ç»´åº¦é”™è¯¯ï¼‰ï¼š{e}")
                continue

    print(f"æŽ¨ç†å®Œæˆï¼Œå…±æˆåŠŸå¤„ç† {len(all_preds)} å¼ æ ·æœ¬")
    return np.array(all_preds), np.array(all_labels), np.array(all_probs), all_images


def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):
    """ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µï¼ˆæ”¯æŒ seaborn æˆ– sklearnï¼‰"""
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(8, 6))
    if _HAS_SEABORN:
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                    xticklabels=class_names, yticklabels=class_names, ax=ax)
        ax.set_xlabel("Predicted Label")
        ax.set_ylabel("True Label")
    else:
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
        disp.plot(ax=ax, cmap="Blues", values_format="d")
        ax.set_title("Confusion Matrix")

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"Confusion matrix saved to {save_path}")
    plt.close()


def visualize_cases(images, labels, preds, probs, class_names,
                    correct=True, num_samples=10, save_path=None, seed=42):
    """å¯è§†åŒ–æ­£ç¡®æˆ–é”™è¯¯çš„æ ·ä¾‹ï¼ˆä¿å­˜å›¾ç‰‡ï¼‰ã€‚images ä¸º CPU tensor listï¼Œlabels/preds/probs ä¸º np.array"""
    random.seed(seed)
    indices = np.where(labels == preds)[0] if correct else np.where(labels != preds)[0]
    if len(indices) == 0:
        print("No samples found for visualization (correct=%s)." % correct)
        return

    num_samples = min(num_samples, len(indices))
    selected = random.sample(indices.tolist(), num_samples)

    cols = min(5, num_samples)
    rows = (num_samples + cols - 1) // cols
    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))
    if rows == 1 and cols == 1:
        axes = np.array([axes])
    axes = axes.flatten()

    # å¸¸ç”¨ ImageNet å½’ä¸€åŒ–å‚æ•°ï¼ˆå¦‚ä½ ä½¿ç”¨ä¸åŒå‚æ•°è¯·ä¿®æ”¹ï¼‰
    # æ³¨æ„ï¼šæ‚¨çš„é…ç½®æ–‡ä»¶ä½¿ç”¨çš„æ­£æ˜¯ ImageNet mean/std
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)

    for i, ax in enumerate(axes):
        if i >= num_samples:
            ax.axis("off")
            continue
        idx = selected[i]
        img = images[idx] # tensor C,H,W on CPU
        if isinstance(img, torch.Tensor):
            # åå½’ä¸€åŒ–ï¼ˆå‡è®¾å›¾åƒå·²ç»è¢« normalizedï¼‰
            try:
                img_disp = img * std + mean
            except Exception:
                img_disp = img
            img_disp = img_disp.clamp(0, 1).permute(1, 2, 0).numpy()
        else:
            # å¦‚æžœ image æ˜¯ PIL æˆ– numpy
            img_disp = np.array(img)

        ax.imshow(img_disp)
        true_label = class_names[int(labels[idx])]
        pred_label = class_names[int(preds[idx])]
        
        # å¤„ç† probs çš„ç»´åº¦ï¼Œç¡®ä¿èƒ½æ­£ç¡®å–åˆ°ç½®ä¿¡åº¦
        if probs.ndim == 2:
            conf = float(probs[idx][int(preds[idx])]) * 100.0
        elif probs.ndim == 1: # äºŒåˆ†ç±» (N,) åªæœ‰ä¸€åˆ—æ¦‚çŽ‡
            conf = float(probs[idx]) * 100.0
        else:
            conf = 0.0
        
        color = "green" if correct else "red"
        ax.set_title(f"T:{true_label}\nP:{pred_label}\nConf:{conf:.1f}%", color=color, fontsize=9)
        ax.axis("off")

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"{'Correct' if correct else 'Error'} cases saved to {save_path}")
    plt.close()


def analyze_error_statistics(labels, preds, probs, class_names, output_dir=None):
    """æ‰“å°å¹¶ä¿å­˜åˆ†ç±»æŠ¥å‘Šä¸Žä½Žç½®ä¿¡åº¦ç»Ÿè®¡"""
    labels = np.array(labels, dtype=int)
    preds = np.array(preds, dtype=int)
    probs = np.array(probs, dtype=float)

    # å¤„ç†äºŒåˆ†ç±» probs åªæœ‰ä¸€ç»´çš„æƒ…å†µ
    if probs.ndim == 1:
        # max_probs ä¿æŒä¸å˜
        max_probs = probs
    else:
        # å¤šåˆ†ç±» max_probs ä¿æŒä¸å˜
        max_probs = np.max(probs, axis=1)

    report = classification_report(labels, preds, target_names=class_names, digits=4)
    print("\n" + "="*60)
    print("Classification Report:")
    print(report)
    if output_dir:
        with open(os.path.join(output_dir, "classification_report.txt"), "w") as f:
            f.write(report)
        print(f"Saved classification report to {os.path.join(output_dir, 'classification_report.txt')}")

    # per-class summary
    print("\nPer-class summary:")
    for i, cname in enumerate(class_names):
        mask = labels == i
        total = np.sum(mask)
        correct = np.sum((labels == preds) & mask)
        acc = 100.0 * correct / total if total > 0 else 0.0
        print(f" Â {cname}: total={total}, correct={correct}, acc={acc:.2f}%")
        # åˆ—å‡ºè¢«é¢„æµ‹ä¸ºå…¶ä»–ç±»åˆ«çš„è®¡æ•°
        if total > 0:
            for j, other in enumerate(class_names):
                if i == j: continue
                cnt = np.sum(preds[mask] == j)
                if cnt > 0:
                    print(f" Â  Â mis -> {other}: {cnt}")

    # ä½Žç½®ä¿¡åº¦ç»Ÿè®¡ï¼ˆé˜ˆå€¼ 0.8ï¼‰
    low_conf_mask = max_probs < 0.8
    low_count = int(np.sum(low_conf_mask))
    total_count = len(labels)
    acc_low = 100.0 * np.mean(labels[low_conf_mask] == preds[low_conf_mask]) if low_count > 0 else 0.0
    print("\n" + "="*60)
    print(f"Low confidence predictions (<80%): {low_count} / {total_count}")
    print(f"Accuracy on low-confidence set: {acc_low:.2f}%")
    if output_dir:
        with open(os.path.join(output_dir, "low_confidence_stats.txt"), "w") as f:
            f.write(f"low_count={low_count}\n")
            f.write(f"total_count={total_count}\n")
            f.write(f"accuracy_on_low_confidence={acc_low:.4f}\n")
        print(f"Saved low-confidence stats to {os.path.join(output_dir, 'low_confidence_stats.txt')}")

    return {
        "low_count": low_count,
        "total_count": total_count,
        "acc_low": acc_low
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Analyze model errors (confusion matrix, visualize cases, low-conf stats)")
    parser.add_argument("--cfg", required=True, help="Path to cfg_effective.json (config)")
    parser.add_argument("--checkpoint", required=True, help="Model checkpoint path")
    parser.add_argument("--output", default="error_analysis", help="Output folder")
    parser.add_argument("--device", default="cuda", help="Device (cuda or cpu)")
    parser.add_argument("--num_samples", type=int, default=15, help="Number of examples to visualize for correct / error")
    args = parser.parse_args()

    os.makedirs(args.output, exist_ok=True)

    model, dataloader, ckpt, class_names = load_model_and_data(args.cfg, args.checkpoint, device=args.device)
    preds, labels, probs, images = analyze_predictions(model, dataloader, class_names, device=args.device)

    # 1) æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(labels, preds, class_names, save_path=os.path.join(args.output, "confusion_matrix.png"))

    # 2) å¯è§†åŒ–é”™è¯¯ä¸Žæ­£ç¡®æ ·ä¾‹
    visualize_cases(images, labels, preds, probs, class_names, correct=False, num_samples=args.num_samples,
                    save_path=os.path.join(args.output, "error_cases.png"))
    visualize_cases(images, labels, preds, probs, class_names, correct=True, num_samples=args.num_samples,
                    save_path=os.path.join(args.output, "correct_cases.png"))

    # 3) ä½Žç½®ä¿¡åº¦ç»Ÿè®¡ä¸Ž classification report
    stats = analyze_error_statistics(labels, preds, probs, class_names, output_dir=args.output)

    print("Error analysis finished. Outputs in:", args.output)